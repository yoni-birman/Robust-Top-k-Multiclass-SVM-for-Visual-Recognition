{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster.k_means_ import _validate_center_shape, _tolerance, _labels_inertia, _init_centroids, _check_sample_weight\n",
    "from sklearn.metrics.pairwise import pairwise_distances_argmin_min\n",
    "from sklearn.utils.extmath import row_norms, squared_norm\n",
    "from sklearn.utils import check_array, check_random_state, as_float_array\n",
    "from sklearn.utils.validation import FLOAT_DTYPES\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.cluster import _k_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subspace_k_means(X, n_clusters, sample_weight=None, init='k-means++', n_init=10, max_iter=300, tol=1e-4, tol_eig=-1e-10, verbose=False, random_state=None, copy_x=True, n_jobs=1, return_n_iter=False):\n",
    "    \n",
    "    if sp.issparse(X):\n",
    "        raise ValueError(\"SubspaceKMeans does not support sparse matrix\")\n",
    "    if n_init <= 0:\n",
    "        raise ValueError(\"Invalid number of initializations. n_init=%d must be bigger than zero.\" % n_init)\n",
    "    random_state = check_random_state(random_state)\n",
    "    if max_iter <= 0:\n",
    "        raise ValueError('Number of iterations should be a positive number, got %d instead' % max_iter)\n",
    "    X = as_float_array(X, copy=copy_x)\n",
    "    tol = _tolerance(X, tol)\n",
    "    \n",
    "    # Validate init array\n",
    "    if hasattr(init, '__array__'):\n",
    "        init = check_array(init, dtype=X.dtype.type, copy=True)\n",
    "        _validate_center_shape(X, n_clusters, init)\n",
    "        if n_init != 1:\n",
    "            warnings.warn(\n",
    "                'Explicit initial center position passed: performing only one init in k-means instead of n_init=%d'% n_init, RuntimeWarning, stacklevel=2)\n",
    "            n_init = 1\n",
    "    \n",
    "    # subtract of mean of x for more accurate distance computations\n",
    "    X_mean = X.mean(axis=0)\n",
    "    # The copy was already done above\n",
    "    X -= X_mean\n",
    "    \n",
    "    if hasattr(init, '__array__'):\n",
    "        init -= X_mean\n",
    "        \n",
    "    # precompute squared norms of data points\n",
    "    x_squared_norms = row_norms(X, squared=True)\n",
    "    \n",
    "    best_labels, best_inertia, best_centers = None, None, None\n",
    "    seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init)\n",
    "    \n",
    "    if n_jobs == 1:\n",
    "        # For a single thread, less memory is needed if we just store one set\n",
    "        # of the best results (as opposed to one set per run per thread).\n",
    "        for it in range(n_init):\n",
    "            # run a k-means once\n",
    "            labels, inertia, centers, n_iter_ = subspace_kmeans_single(X, sample_weight, n_clusters, init=init, max_iter=max_iter, tol=tol, tol_eig=tol_eig, verbose=verbose, x_squared_norms=x_squared_norms, random_state=seeds[it])\n",
    "            # determine if these results are the best so far\n",
    "            if best_inertia is None or inertia < best_inertia:\n",
    "                best_labels = labels.copy()\n",
    "                best_centers = centers.copy()\n",
    "                best_inertia = inertia\n",
    "                best_n_iter = n_iter_            \n",
    "    else:\n",
    "        # parallelisation of k-means runs\n",
    "        results = Parallel(n_jobs=n_jobs, verbose=0)(delayed(subspace_kmeans_single)(X, sample_weight, n_clusters, init=init, max_iter=max_iter, tol=tol, tol_eig=tol_eig, verbose=verbose, x_squared_norms=x_squared_norms, random_state=seed)for seed in seeds)        \n",
    "        # We Change seed in delayed function above to ensure variety\n",
    "        # Get results with the lowest inertia\n",
    "        labels, inertia, centers, n_iters = zip(*results)\n",
    "        best = np.argmin(inertia)\n",
    "        best_labels = labels[best]\n",
    "        best_inertia = inertia[best]\n",
    "        best_centers = centers[best]\n",
    "        best_n_iter = n_iters[best]\n",
    "        \n",
    "    if not copy_x:\n",
    "        X += X_mean\n",
    "    best_centers += X_mean\n",
    "\n",
    "    if return_n_iter:\n",
    "        return best_centers, best_labels, best_inertia, best_n_iter\n",
    "    else:\n",
    "        return best_centers, best_labels, best_inertia\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (cell_name, line 85)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"cell_name\"\u001b[1;36m, line \u001b[1;32m85\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "def subspace_kmeans_single(X, sample_weight, n_clusters, init='k-means++', max_iter=300, tol=1e-4, tol_eig=-1e-10, verbose=False, x_squared_norms=None, random_state=None):\n",
    "    random_state = check_random_state(random_state)\n",
    "    sample_weight = _check_sample_weight(X, sample_weight)\n",
    "\n",
    "    best_labels, best_inertia, best_centers = None, None, None\n",
    "    # init\n",
    "    centers = _init_centroids(X, n_clusters, init, random_state=random_state, x_squared_norms=x_squared_norms)\n",
    "    if verbose:\n",
    "        print(\"Initialization complete\")\n",
    "        # Allocate memory to store the distances for each sample to its\n",
    "    # closer center for reallocation in case of ties\n",
    "    distances = np.zeros(shape=(X.shape[0],), dtype=X.dtype)\n",
    "\n",
    "    # === Beginning of original implementation of initialization ===\n",
    "\n",
    "    # Dimensionality of original space\n",
    "    d = X.shape[1]\n",
    "\n",
    "    # Set initial V as QR-decomposed Q of random matrix\n",
    "    rand_vals = random_state.random_sample(d ** 2).reshape(d, d)\n",
    "    V, _ = np.linalg.qr(rand_vals, mode='complete')\n",
    "\n",
    "    # Set initial m as d/2\n",
    "    m = d // 2\n",
    "\n",
    "    # Scatter matrix of the dataset in the original space\n",
    "    S_D = np.dot(X.T, X)\n",
    "\n",
    "    # Projection onto the first m attributes\n",
    "    P_C = np.eye(m, M=d).T\n",
    "\n",
    "    # === End of original implementation of initialization ===\n",
    "\n",
    "    # iterations\n",
    "    for i in range(max_iter):\n",
    "        centers_old = centers.copy()\n",
    "\n",
    "    # === Beginning of original implementation of E-step of EM ===\n",
    "\n",
    "    X_C = np.dot(np.dot(X, V), P_C)\n",
    "    mu_C = np.dot(np.dot(centers, V), P_C)\n",
    "    labels, _ = pairwise_distances_argmin_min(X=X_C, Y=mu_C, metric='euclidean', metric_kwargs={'squared': True})\n",
    "    labels = labels.astype(np.int32)\n",
    "    \n",
    "    # === End of original implementation of E-step of EM ===\n",
    "    # computation of the means is also called the M-step of EM\n",
    "    centers = _k_means._centers_dense(X, sample_weight, labels, n_clusters, distances)\n",
    "    # === Beginning of original implementation of M-step of EM ===\n",
    "    S = np.zeros((d, d))\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        X_i = X[:][labels == i] - centers[:][i]\n",
    "        S += np.dot(X_i.T, X_i)\n",
    "    Sigma = S - S_D\n",
    "    evals, evecs = np.linalg.eigh(Sigma)\n",
    "    idx = np.argsort(evals)[::1]\n",
    "    V = evecs[:, idx]\n",
    "    m = len(np.where(evals < tol_eig)[0])\n",
    "    if m == 0:\n",
    "        raise ValueError('Dimensionality of clustered space is 0. The dataset is better explained by a single cluster.')\n",
    "    P_C = np.eye(m, M=d).T\n",
    "    inertia = 0.0\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        inertia += row_norms( X[:][labels == i] - centers[:][i], squared=True).sum()\n",
    "    \n",
    "    # === End of original implementation of M-step of EM ===\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Iteration %2d, inertia %.3f\" % (i, inertia))\n",
    "\n",
    "    if best_inertia is None or inertia < best_inertia:\n",
    "        best_labels = labels.copy()\n",
    "        best_centers = centers.copy()\n",
    "        best_inertia = inertia\n",
    "\n",
    "    center_shift_total = squared_norm(centers_old - centers)\n",
    "    \n",
    "    if center_shift_total <= tol:\n",
    "        if verbose:\n",
    "            print(\"Converged at iteration %d: center shift %e within tolerance %e\" % (i, center_shift_total, tol))\n",
    "        break\n",
    "        \n",
    "    if center_shift_total > 0:\n",
    "        # rerun E-step in case of non-convergence so that predicted labels\n",
    "        # match cluster centers\n",
    "        best_labels, best_inertia = \\\n",
    "            _labels_inertia(X, x_squared_norms, best_centers,\n",
    "                            precompute_distances=False,\n",
    "                            distances=distances)\n",
    "\n",
    "    return best_labels, best_inertia, best_centers, i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subspace k-Means clustering <br>\n",
    "    Read more in\n",
    "    Mautz, Dominik, et al.\n",
    "    \"Towards an Optimal Subspace for K-Means.\"\n",
    "    Proceedings of the 23rd ACM SIGKDD\n",
    "    International Conference on Knowledge Discovery and Data Mining. ACM, 2017.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_clusters : int, optional, default: 8\n",
    "        The number of clusters to form as well as the number of\n",
    "        centroids to generate.\n",
    "    init : {'k-means++', 'random' or an ndarray}\n",
    "        Method for initialization, defaults to 'k-means++':\n",
    "        'k-means++' : selects initial cluster centers for k-mean\n",
    "        clustering in a smart way to speed up convergence. See section\n",
    "        Notes in k_init for more details.\n",
    "        'random': choose k observations (rows) at random from data for\n",
    "        the initial centroids.\n",
    "        If an ndarray is passed, it should be of shape (n_clusters, n_features)\n",
    "        and gives the initial centers.\n",
    "    n_init : int, default: 10\n",
    "        Number of time the k-means algorithm will be run with different\n",
    "        centroid seeds. The final results will be the best output of\n",
    "        n_init consecutive runs in terms of inertia.\n",
    "    max_iter : int, default: 300\n",
    "        Maximum number of iterations of the k-means algorithm for a\n",
    "        single run.\n",
    "    tol : float, default: 1e-4\n",
    "        Relative tolerance with regards to inertia to declare convergence\n",
    "    tol_eig : float, default: -1e-10\n",
    "        Absolute tolerance with regards to eigenvalue of V to assume as 0\n",
    "    verbose : int, default 0\n",
    "        Verbosity mode.\n",
    "    random_state : int, RandomState instance or None, optional, default: None\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`.\n",
    "    copy_x : boolean, default True\n",
    "        When pre-computing distances it is more numerically accurate to center\n",
    "        the data first.  If copy_x is True, then the original data is not\n",
    "        modified.  If False, the original data is modified, and put back before\n",
    "        the function returns, but small numerical differences may be introduced\n",
    "        by subtracting and then adding the data mean.\n",
    "    n_jobs : int\n",
    "        The number of jobs to use for the computation. This works by computing\n",
    "        each of the n_init runs in parallel.\n",
    "        If -1 all CPUs are used. If 1 is given, no parallel computing code is\n",
    "        used at all, which is useful for debugging. For n_jobs below -1,\n",
    "        (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one\n",
    "        are used.\n",
    "    Attributes\n",
    "    ----------\n",
    "    cluster_centers_ : array, [n_clusters, n_features]\n",
    "        Coordinates of cluster centers\n",
    "    labels_ :\n",
    "        Labels of each point\n",
    "    inertia_ : float\n",
    "        Sum of distances of samples to their closest cluster center.\n",
    "    m_ : integer\n",
    "        Dimensionality of the clusterd space\n",
    "    V_ : float ndarray with shape (n_features, n_features)\n",
    "        orthonormal matrix of a rigid transformation\n",
    "    feature_importances_ : array of shape = [n_features]\n",
    "        The transformed feature importances\n",
    "        (the smaller, the more important the feature)\n",
    "        (negative value (< tol_eig): feature of clustered space)\n",
    "        (positive value (>= tol_eig): feature fo noise space).\n",
    "    n_iter_ : int\n",
    "        Number of iterations corresponding to the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubspaceKMeans(KMeans):\n",
    "    def __init__(self, n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=1e-4, tol_eig=-1e-10, verbose=0, random_state=None, copy_x=True,  n_jobs=1):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.init = init\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.tol_eig = tol_eig\n",
    "        self.n_init = n_init\n",
    "        self.verbose = verbose\n",
    "        self.random_state = random_state\n",
    "        self.copy_x = copy_x\n",
    "        self.n_jobs = n_jobs\n",
    "        return\n",
    "    def fit(self, X, y=None, sample_weight=None):\n",
    "        if sp.issparse(X):\n",
    "            raise ValueError(\"SubspaceKMeans does not support sparse matrix\")\n",
    "        random_state = check_random_state(self.random_state)\n",
    "        self.cluster_centers_, self.labels_, self.inertia_, self.n_iter_ = subspace_k_means(X, n_clusters=self.n_clusters, sample_weight=sample_weight, init=self.init, n_init=self.n_init, max_iter=self.max_iter, tol=self.tol, tol_eig=self.tol_eig, verbose=self.verbose, random_state=random_state, copy_x=self.copy_x, n_jobs=self.n_jobs, return_n_iter=True)\n",
    "        \n",
    "        # === Beginning of original implementation of additional info ===\n",
    "        d = X.shape[1]\n",
    "        S_D = np.dot(X.T, X)\n",
    "        S = np.zeros((d, d))\n",
    "        \n",
    "        for i in range(self.n_clusters):\n",
    "            X_i = X[:][self.labels_ == i] - self.cluster_centers_[:][i]\n",
    "            S += np.dot(X_i.T, X_i)\n",
    "        Sigma = S - S_D\n",
    "        self.feature_importances_, self.V_ = np.linalg.eigh(Sigma)\n",
    "        self.m_ = len(np.where(self.feature_importances_ < self.tol_eig)[0])\n",
    "        # === End of original implementation of additional info ===\n",
    "\n",
    "        return self        \n",
    "\n",
    "    def _check_test_data(self, X):\n",
    "        X = check_array(X, accept_sparse=False, dtype=FLOAT_DTYPES)\n",
    "        n_samples, n_features = X.shape\n",
    "        expected_n_features = self.cluster_centers_.shape[1]\n",
    "        if not n_features == expected_n_features:\n",
    "            raise ValueError(\n",
    "                \"Incorrect number of features. \"\n",
    "                \"Got %d features, expected %d\" %\n",
    "                (n_features, expected_n_features)\n",
    "            )\n",
    "        return as_float_array(X, copy=self.copy_x)\n",
    "    \n",
    "    def _transform(self, X):\n",
    "        return np.dot(X, self.V_)\n",
    "    \n",
    "    def inverse_transform(self, X, copy=None):\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit function <br>\n",
    "Compute subspace k-Means clustering.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like or sparse matrix, shape=(n_samples, n_features)\n",
    "            Training instances to cluster.\n",
    "        y : Ignored\n",
    "            not used, present here for API consistency by convention.\n",
    "        sample_weight : array-like, shape (n_samples,), optional\n",
    "            The weights for each observation in X. If None, all observations\n",
    "            are assigned equal weight (default: None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
